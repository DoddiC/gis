(Transcribed by TurboScribe.ai. Go Unlimited to remove this message.)

I've heard you use this term in 24 hours, and I know that I've heard you use this before, but I forget what it references. Because I know it's not actual birds shitting on our application. Correct.

It's referring to the documents that are sitting around in couch bays, in back of sticks fields, and nobody cares. And if we take a step back and look at this, every single one of these, quote, unquote, bird poop is waste, right? You have the, whether it is an inspector or the repair crew, they are out there, they spend all this time filling up this form on our precious applications, and it's going nowhere, right? It's just sitting in some kind of a black hole, and nobody cares, right? Yeah, I guess I'm going to do a short answer to your question, or something like that. Every payroll should be zero-paid documents at the end of the day.

Nothing should be sitting in a state in couch bays. Great. That's what I want to hear, right? And then the ultimate test will be go live, right? So if indeed that's what's happening, this zero failures, then you're going to be like, hey Angus, I'm ready for this stuff to flip over to someone's team, right? So let's cut that out and see if that's what happens.

In terms of what you were trying to address earlier about, hey, if you're talking about monitoring for the new features that you guys are building, specifically around application performance, crashes, and stuff like that, I need to, I don't want to waste anybody's time. I'm not really the best person to talk to on that one. In my opinion, I think developers such as Rocky and Colin are in a much better position to have good insights into what makes sense to log and monitor from a application performance perspective.

Ron, you can probably provide some guidance as well, but I think a lot of that is- That was already given, Angus, so Zing already put the logs around it. Okay. And so the crashes, everything that comes to the app, the, yeah, that's something that you get out of the box.

But I can look at the breadcrumbs, also I think was added, but that's something that I can confirm. But the logs also- What is, what are breadcrumbs? Breadcrumbs basically, in app dynamics, you can basically track down to what are the sequence of events that are happening, like where do you click on and what happened after that. It gives you that trail before anything happens, before it crashes, before it takes a long time to respond and so on.

Okay. So in that case, does it, are the breadcrumbs sort of generated after the fact, after a crash? It says, this is the stuff that happened leading up to that? Yeah. Yeah.

Okay. And so is there any wisdom in, in trying to sort of target certain features and maybe even just sort of give kind of regular reporting on something that we know might be problematic, like their memory handling, right? So if a person is, for instance, running around doing a patrol on 20 different work orders at once, and they're going through, they're circling all these assets, is there any wisdom in just kind of doing some level of monitoring, as an example, on the process of sort of circling or lassoing and then submitting thousands of assets? And just kind of tracking that, so we know if that's going to... Yeah, exactly, right? So that's what I was referring to. Those are the things that should be part of your excellent criteria when you're writing a story.

Hey, I want to track this information, like when the user selects five work orders, or 10 work orders, I want to know how many work orders he selected, how many assets he selected to submit the document and so on. So that way, Zyng can basically make sure that he created those respective logs. Wait, do we want to log this in Symbologic? I don't think so.

At any time, do you just want to... Like the breadcrumbs on the selection... No, no, no, the breadcrumbs in Appdb are not exactly the breadcrumbs. I'm talking about the number of work orders that you select, the number of assets found on the map or not found on the map, that kind of information has to be logged into Symbol. The number of work orders found on the map? So, in Petrols, it's a little different, right? In Petrols, the user can select more than one work order.

They'll be able to use up to 30 work orders at a time. Right, and also like in Power, they'll say if there are 100 assets, out of 100, one or two assets, you may not be able to find in GIS. Like if he's not able to find out, you need to log that information into Symbol so that we know what are the assets that are in question, right? We can do something related.

That, I think it's overkill, because we're talking about very high volume here. We are. Just having a count is not really going to help diagnose anything.

I know you will keep talking about that. No, no, no, Angus, trust me. We already have that level of logging right now.

We have some set of equipment that are not found in GIS, but they are in SAP. We log that information. That is helping right now.

Oh, in InSpec? Yeah, in InSpec, yes. So similarly, we have to do the same thing for Petrol, so that we can certainly create some kind of alerts based on that, saying that, hey, these new assets are not there. We can send that potential information back to the planners or asset strategists so that they can be aware of when that trading is collapsed.

We did not get any alerts on it, but again, something that potentially can do. But if we don't log that information, there is no way for us to know what is missing, what is not really there. So you need to rely on this level, certainly on what InSpec is going to say at the end of the day.

You need to have logs. Yeah, so Ram, these logs that you're talking about, so say we do get a little bit more granular with the logs, and we're keeping, as they go throughout their day, and they are submitting patrols on hundreds and hundreds of assets, potentially thousands of assets. In your mind, are we still sort of waiting for a crash to happen before those logs are presented? No, no, no.

So, no, no, no. It's like, hey, I want to have this information regardless of how the app is working, or how the app is trading, right? Yeah. So that means you need to determine, like, hey, I want to have a number of work orders.

Just again, I'm giving you examples here, like you can determine what information that you want to have all the time, regardless if the app is working fine or it's crashing. Say, for example, a number of work orders, or a number of assets found, versus a number of assets not found, kind of thing, right? Hey, I want to have this information all the time. So that's something that you can define, right? And also, we have these, that's what I call logs.

On the logs, we have the various levels of logging, like, hey, I want to have this information all the time. But in some cases, I want to log only the warnings, or maybe I want to log only the errors, and that thing, right? So if you can spell those things out, then this thing can code accordingly so that it can see that respective information in some logic, okay? When it comes to the real user monitoring, or the application monitoring thing, which is basically the purpose of the AppDynamics, is going to tell you how well your app is doing, like, when it makes a network call, how well your app is doing when there is a crash, like, how well it's doing when it's trying to draw the halos on the map, like, if there's any errors, like, the application is not responding, those kind of events. Some of those things will come right out of the box from AppDynamics.

Okay. I guess what I'm kind of focusing on is, how do we determine, like, the additional things that need to be added into AppDynamics? Like, or, I guess, first of all, things are gonna come out of the box with AppDynamics. Can we add additional things in AppDynamics? I guess that's the question.

Okay. Yeah, we need to find out what you want exactly. Like, now we have some breadcrumbs already enabled.

Yeah. You can check with the thing, hey, like, maybe something that you are taking advantage of, or, like, something else we need to add, right? Something like that we need to, maybe once your application is in QA, maybe at that point of time, we can review the AppDynamics and also Sumo logs, and see, like, whether it's giving the information that what we're looking for. If it's not, then we need to add a little bit more to it.

Okay. So, maybe it might be helpful to kind of do an exercise here real quick. Maybe, let me see if I can share my other screen.

Okay. Again. Something's not new.

Screen recording. Nope. Not you.

Okay. Not recognizing my iPad for some reason. Oh, the QuickTime thing? Yeah.

Yeah, it's stupid like that. Mine's interesting. We need to do a reboot before that works.

Yeah. I think I just found it. Yeah, see something that should, like, something like Flask.

I think we have to give the other dashboard. Yeah, there, okay, I just found it. So, here we go.

Here's the Inspect app, what it looks like. Now, Angus, this might look sort of familiar to you because it looks a little bit more like transmission now with the split screen, and you can see there's multiple work orders down the side, but you can also see that, similar to what we're doing in transmission, is that you can enable a lot of, oops, I'm gonna focus on San Francisco work orders right now because I have that map downloaded, but you can enable a lot of work orders, and we're actually giving people the ability to enable up to 30 work orders at once for helicopter patrols, which could potentially do a huge number of patrols within a very short period of time. So, in this scenario, maybe just kind of help me understand what we would already be monitoring versus what we might need to add coming out of AppDynamics.

So, we've got these patrols, halos. I wanna select the assets that I have patrolled, and I can draw a little, it's been misbehaving on me for a little while now. I need to kill my app.

Is that misbehavior in ANR stuff? Is this a what? Is the misbehavior that you're experiencing in ANR in the app? I don't know what that means. So, ANR basically means Application Not Responding, and James, when you click on something, you see some kind of a, mostly it's called coronavir, the spinning wheel. Yeah, yeah.

It does take quite some time before you get that control back, before you click on anything. Okay. But here we go now.

So, I mean, maybe that's a good example of one thing we should be monitoring, is how frequently people have to kill their application in the background, and what was going on prior to that. Yeah, I think that's definitely a problem. I don't think we've talked about it, monitoring the kill itself, but the app start.

Definitely could be a good thing to monitor. Yeah, we are tracking some of these things, James, like when an app is crashed and they have a launch check kind of thing. Oh yeah, they crash the upgrade, right? Yeah, tracking something like a lost productivity.

Say like an app is crashed because of this on the next time until they re-launch the app. We consider that as a productivity loss. We are reflecting that.

Yeah. Do you know, Ron, I don't wanna go too far down this rabbit hole, but do you know if it's only when the app itself crashes, or if it's when they actually sort of kill the application in the background, or does it look like the same thing from a monitoring perspective? I don't think so. The app didn't crash on me just now.

I had to kill it in order to get it going again. It'll look different. They're statues.

Okay. Yeah. So maybe that might be a good one to just start with, is like how frequently are people killing the application from the background, and maybe what was going on right prior to them having to kill it? Yeah.

I'm gonna do a different experiment, like in which event it's getting called and then we can force close it. Yeah. No? Because I don't always trust that, you know, the crashes that we're getting are necessarily telling us that the app died.

At least I would say 90% of the time we get it. That's the remaining 10% that I was in question. Yeah.

So, yeah. No, Ron, I mean, this is a valid point, right? Like oftentimes, you know, the app's not behaving. People just don't give a QoD app restart, right? This is a little bit gray.

It's very hard for us to like, you know, really tell how often this is happening because it's pretty much indistinguishable from a normal QoD app. And then you relaunch later. It's pretty much indistinguishable from a metric perspective.

Right. Again, so on this monitoring, I think there's a lot of things, right? I think, like, let's focus on what we really want to track initially and see, like, from there. I think, again, we cannot get everything right on the day one.

It will be- No, you're absolutely right. I do want to try to prioritize things, but I want to kind of put a pin in that one because that might be something valuable kind of across the board for all of the product teams. But in this scenario, okay, so we are, I've gone through, I've selected, you know, whatever this number of assets is, and then I hit next.

I attest that I have actually patrolled all of those and I submit progress. I think the monitoring that came out of the box, as in, like, hypothetically, what is the monitoring that comes out of the box for just that action? Was there anything or? Yeah, so there is something like, you know, the breadcrumbs enabled from the beginning, like, where you selected the work orders, where it is, like, it showed you all the assets on the map. That's also another action.

And you click on the, like, the sediment, a glass of ink. All those, like, you know, breadcrumbs is enabled. You see all of that in AppDynamics.

Okay. So James, like, regardless of, like, you know, which tools are we using, like, whether it's, you know, AppDynamics or these, like, you know, application level type stuff, or Simulogic or specific data logs, it's basically an iterated approach where the developer is, you know, doing stuff like, you know, you're doing right now, and then it's kind of like comparing that against, hey, what logs are we capturing here? And then if they kind of, like, you know, notice weird stuff going on, they kind of, like, you know, are we capturing this in our monitoring tools? If not, then, hey, where do we need to inject some code to capture this stuff? I mean, it's basically an iterated approach, but at the end of the day, you can't avoid having your developers and get their hands wet into really looking into the tools. Yeah, you're absolutely right, and I'm just trying to sort of do the due diligence and identify up front, like, if there are things that we think are gonna be valuable, you know, can we add those things as part of day one? So we are going to get cratches, right? And knowing that our developers are gonna have to be, are gonna have to kind of scramble initially to resolve things, can we at least give them, you know, a set of additional information that might help zero in on what the problem was and whether that is, you know, in this case, you know, attached to this function, we, you know, we have a certain number of halos that are highlighted.

There was a certain number of halos that were selected and submitted. So there's the patrol type document was generated and maybe it was this size, and then maybe take a snapshot of, you know, the memory and how the memory is handling. So we can get a little bit of a better sense of the help, but I mean, I'm trying really hard not to be in a position where we get out of the gates on day one, cratches start happening, and we have to just release a new, immediately release a new version of the app to try to even understand what the cratches are.

Yeah, in QA, have you guys noticed anything? I mean, sure, but also QA is kind of not production. We're being more diligent about doing performance level testing on, in the application, because we know that there's going to be a much higher volume of assets, but I'm not relying on QA to give us a totally full picture of what's going to happen in production. I agree, but like, you know, you guys kind of think about like, how would we normally do this stuff, right? Would be like, honestly, a big part of our problem, right, your problem statement that you, you know, you, during your intro of this call itself, all of these are non-issues.

We actually have a good QA process. At the end of the day, our QA is insufficient. Well, you are right.

And I wish I could say it's going to be a lot better for the initial release, but I can promise you it's not going to be. No, I understand, but this is the state we're in, right? We have not been able to change that state. It's not going to change in the next three months.

It's not going to change in the next year. So we understand that, but my point is regardless, right? The iterative process, we start with QA, right? We start looking into the monitoring outcomes from the QA process, right? We do it wherever we can while we're doing QA, right? And then because of our insufficient QA system, not being able to truly validate what the production environment is going to be, we're going to see differences, right? So to make that gap, right? What we're basically doing is like, from a release strategy perspective, we've really been just using pilots at our extended QA, that's literally what it is. But obviously we want to kind of keep that damaged to as a minimal amount as possible for our production, quote unquote, QA users.

And yeah, unfortunately we don't. And unfortunately we don't have pilot as an option for this go-live because it's an entirely new process and people have not been trained on it at all. And it's going to be available in production on January 5th, but people aren't effectively allowed to use it until January 16th, at which point they will be starting the inspections.

So unfortunately, like that pilot phase that might tell us some useful information is not going to be very helpful for us. Thanks, John. If by that you mean we're just kind of going in blind? No, what I mean by that is on 16th, the entire community is going to be on this process with no prior experience and no prior metrics.

Yeah. So they're being trained on the 14th and 15th. On the 16th, they start working.

That is also Martin Luther King weekend. So they get time and a half or double time. And most people work through the weekend because of that.

So we're expecting like an enormous volume of work, literally that first weekend after training. Is there a certain level of testing we can do in production without actually making submissions? We can work with the work planners, create like dummy work orders whose intention is to be removed from SAP afterwards. We have done that in the past for training purposes and stuff like in testing purposes.

So that is a potential option, yeah. But I think my concern about that where I think we really need to be careful is that I think that what we need to be doing is running automation on test automation to really suss out, like, can our application handle this? And my understanding is that running automation, test automation against production is the dumbest mistake anybody can make in their life. Of course.

No, no, no. I've never been on your end of this, but I don't want you to put you in that position. No, no, no, no, no, no, no, no, no, no, no, no, yeah.

Yeah. Because here at the end of the day, right, our key environment is not reflective of production. There's gonna be a lot of crap that we're just not gonna catch in QA, right? Yeah, that's true, and I'm... Right? So when we go to product, we know for a fact that shit is gonna happen, right? It's not QA.

We know for a fact that's gonna happen. But the only way for us to mitigate, right, is to get into prod as early as possible and start getting metrics from, and actually start kind of fixing stuff before it actually goes to the user's end. So we, like, in my opinion, we need to do prod testing as early as possible.

Yeah, and so, it's not that easy. So, I mean, I agree with you, and I wish that it was something that we could do, but the testing would have to be manual. Creating a huge volume of work that would actually provide the data we need to identify problems, it would probably have to be done with, like, a human with a literal iPad.

Well, no, can we not just go with, like, whatever the planners were going to go live with on January 6th, 16th, right? Just assign them to our developers and our QA testers in production. Again, the idea is we're not actually gonna submit anything, but, you know, a lot of the issues will happen before your submission. Yes and no.

So we can give it to the few developers that are actually in the area and have a physical iPad. Having the QA engineers do it, they're on remote devices, and I don't trust that we're gonna get the right data from them. If they do that, we can get some data, but I don't think it's gonna be the right data, or there will be things that are not functioning, like GPS is, like, not, in a virtualized machine, it's not gonna be very useful.

Yeah, the lab devices are not gonna be useful in this type of, yeah, yeah, yeah. Yeah, so between, you know, between January 5th, when it goes into production, and, you know, January 16th, when the inspectors start to use it, we could, that could potentially be a window where a couple of our developers are doing production testing on, you know, on mock work orders, like, actually, like, mirror images of what real work orders, but ones that are planning to be destroyed afterwards. So that could be done, but I think- That's a lot of overhead work.

It's a lot of overhead work, and I still think, you know, if we can identify, you know, like, gaps in our monitoring that would help us to sort of be in a better position even prior to that testing, I think it would, I think it would be valuable. So that's really what I'm trying to identify here, is, like, are there gaps in our monitoring that are, you know, will help us not be so blind once we get into production? Yeah, that's fair, and that makes sense, but, you know, for those gaps, right, again, we're still kind of, like, more interested in, hey, the app is not behaving correctly. There's some weird issue that's happening in the app that may be leading to, like, you know, what you were just experiencing where you had to kill the app and relaunch or, you know, an actual crash, right? Right, so we're looking at, like, bugs, right? Like, the app is not handling a certain condition.

The app is not handling-

(This file is longer than 30 minutes. Go Unlimited at TurboScribe.ai to transcribe files up to 10 hours long.)