(Transcribed by TurboScribe.ai. Go Unlimited to remove this message.)

CMDB. I don't know if you guys are able to see my screen. All right, so I think, Angus, I've shared with you a little bit on what this Lambda does, right? I think this is still, this is the Lambda that we've been, or the integration to that CMDB database that we've been talking about for patrols that was using that Microsoft SQL database.

We weren't sure on how to make that connection from our AWS services because it required that Windows application. Oh, I vaguely remember that. Before I left, we discussed about this.

Yeah, the actual architecture. But what this does is, so for the patrol project, right, we have two systems in play there. We have the engage application and then we also have the inspect application, right? And on the engage application is essentially our assignment tool, right? Where we assign work orders to someone and then they will go out and do the actual inspection or patrol in this case. 

The patrol project today use a, it's all on paper with the exception of this CMDB database where they do essentially all of that same assignment where they assign work orders to different users. I think I have a screenshot of what that looks like, unfortunately. Let's see. 

So they're currently using CMDB to do these assignments. But it's, yeah, it's just a database. They have all the necessary data that they need to do the work order. 

What is the bad code? Who is the assignee and when is it due, essentially? Wish I had some pictures here to show what that looks like. So the CMDB is referring to the database itself, but there's some kind of a web client that's wrapping around this database for the actual assignment work. This web client here that I'm showing, yeah, there's like a table here.

It looks a little bit like this. It's just a row of tables to add who's going to do what, what order, when is it going to be, when is it done and what it's like to work. So it's just very similar information to what we already have in the engage application.

But the goal with this Lambda is essentially to integrate some of the CMDB data and send that over to the engage applications, because there are some fields that we don't get from SAP or any of our other systems that populates the engage application. And it only exists within CMDB. And one of those fields is a field called month plan, which I think I put in here.

Okay, I'll add that here. But that field is manually inserted by the supervisors. It's called month plan. 

It is essentially a field that tells the supervisor whenever the work order is supposed to be complete. There are similar fields already in engage, like due date, but it's apparently not the same as month plan. So because of that, we had to create this integration, which is essentially just a Lambda. 

It pulls the data from the CMDB database, and then it creates a calc based document. We can already see that in LazyBoy. A call, I think, CMDB engage.

Some .NET CMDB engage, I think. And then engage ingests that CMDB document via Neptune. Engage 6CMDB, the other way around.

Engage CMDB, I guess. Yeah, yeah. It's a doc type. 

We ingest the data, and all we really care about is what's the path map year, what is the SAP order or work order, ID, just in case there's any duplicates or anything like that, what is the map code, and then that month plan, right? We do also capture the modified date to do update the actual data in the CMDB database. Right now, we're pulling this data daily, so nightly, I think at 12 AM. Every night, the Lambda runs, pulls the data, creates a document, or updates the document, and then that's really it.

How is it triggered? Is it an event schedule? Yes, so it's basically a project. Right, but on AWS, there's some kind of event schedule thing, right? Yeah, it's like an event cloud watch, I think it's called. Cloud watch event.

Yeah, event bridge. Cloud event bridge. So all we're worried about is, and I did remove that configuration, but I don't think we really needed that database configuration.

Yeah, all we care about, right, is, let's see, is this full? Maybe the query. So yeah, if it starts the function, we retrieve the parameters. We want to see, so a lot of the data or the R, there are three parameters that it actually, on the parameters there.

So we need to know if those succeeded or if some of those failed, right? It pulls the username, the password, and then this one, I think it's important too, so every time it does a pull, it will update this parameter, so that the next time we do a pull, we only pull the data that was updated after this last. This last pull, basically. So that way, we don't pull the whole database every time.

We're only pulling anything that was modified after the last batch, essentially, which is going to be 24 hours. And then we update that in the parameters that whenever that pull was made. It does the querying.

You can see the last updated timestamp, so we can see that if something is wrong, it executes the query. This is optional, I think, too. We don't necessarily have to show the query.

It could be good for debugging purposes. Execution of the database query. So if anything goes wrong there, we can see.

I think this is just a duplicate of the previous one, so we could probably... I would say remove the query. The reason is it's quite wasteful. Yeah.

Oh, but then... Yeah, yeah. Don't worry too much about this at the moment. It runs once a day.

Yeah. It might run more in the future, because we are thinking of pulling more data, and then also eventually writing to the database. So it is going to be a lot.

So they also want to... Because the assignments, the days being done, both of them seem to be unengaged by the supervisor. So eventually they want to move to engage only, and then have engage update the assignment, and then write to CMDB. Why do we want to keep both in the long term if they want to do all assignments in engage in the future? In the short term, it needs to be like that, because we have four math codes that we need to integrate into the patrols party.

So I think we only have BFA and BFB today, and there's four of them. There's BFD and BFE. And I believe it's only BFA that is being released now in the next coming months.

So the BFB, BFD, and BFE will still continue to be on paper until we migrate those work types as well. And I think the final migration is set to be 2028-29, I think. So it is going to be a couple of years of them operating on both systems.

Yeah, because of the nature of the integration, and that it's like a year at a time. The main problem is that we basically have a month's window every year to swap. Yeah, so in January, they get all the data, they do the planning, and then they sort of do all the assignments for the rest of the year, essentially.

So if we miss that date, then we have to wait another year for us to digitize. So yeah, it's probably going to be very difficult, I don't think we'll need that. Creating a database connection pool, so we make sure the FAT is set up.

We're choosing the parameters for the... Do we have any reach price on any request to the parameter store? Because that stuff, I mean, it doesn't fail all the time. But if, you know, the rate limit happens fairly regularly. I think so, let me see.

Parameters store, oh, I think we just... I can add that, but I don't think we do... I'm just wondering, like, what happens if we have a rate limit failure? Yeah, no, that's a good point. So that's one issue during the initiation part, I'm guessing. It's like, oh, okay, it failed today.

It'll pick up where it left off tomorrow. But if it failed, like, you know, at the end, it's done with everything, but then it fails to update the parameter store for the lab. You don't know the dot tracker big time, right? It's going to reprocess stuff, I'm guessing, on the next run.

I don't know how big a deal that is. Yeah, I think we'll have to talk to the business on that, because I think we only set it for two runs once a day, right? Because we don't think the data is that critical. And I mean, not that they're not critical, it's more that they don't really do updates on the month plan that often.

It's like once every quarter when they do actual planning, and then it sort of stays like that. But let me ask them if there are any scenarios where they want to fetch the data more frequently, or if they can wait, sort of, if they need to fetch the data. Yeah.

But do you think they only make these updates once a month? And why do we even need to do this on a daily basis? Good point. There are other fields, I think, that we're interested in potentially looking at as well. So that's why it's more of a daily thing right now.

Okay. So just in case we're sensitive to future proof a little bit. Okay.

So that's a good, I'll follow up on that. Let's see, frequency option. And then yeah, the configuration I removed already, we don't need that.

The connection query, we want to see how many records we've got. This is just another duplicate. I'll remove that.

We're seeing a lot of people again, so that's why. So the query is against the CMBB database, but what parameters are we passing to CMBB? These ones. So we're only looking for the ID, the top map year, SAP order, month plan, the map code, and the modified data.

There's a lot of other ones. Um, so if I go to... Did the filter is only based on the modified date? Is that right? And there's no other filters. Basically, the CMBB database, the data set is relatively small.

It's like we fetch everything past a certain date. Yeah. Okay.

But there are other... If you go to the actual database, there are other fields that we might potentially want to look at. For example, the urban and rural one eventually we might want to pull in as well. There's like an aerial one, the expected land IP is what we want to also potentially look into.

And whether or not it was an aerial or ground control as well, we haven't decided yet on where that field is going to live. So there are some things that we're eventually might be doing some filters. But the idea is that... The idea is that this is the filter that we use to pull in the data and then engage via Neptune, might do filtering on the map code, for example, or the SAP order.

So once it gets to Neptune, they will do that filtering. Jonathan, you also said this happens via driving, right? Them inspecting from a distance and all. So is that going to be like, you know, is that an enhancement later as well? So that's what we're doing right now with the patrols.

That's sort of what they're... That's the point of the patrol. But they're going to do the inspections from a distance to drive them. But this part for the... So the field usually doesn't really care about the engage part.

That's one of the assignment back office part. So the inspect application actually doesn't see this data. They don't really have to see this data.

Retrieving parameter, same here. Oh, this is the same. What we were watching earlier.

So another important part of the Couchbase. Yeah, there's a couple of... It's a wrapper, right? Basically, you got a wrapper function. And then, like, down below, it's like the detail steps of what we're doing.

Yeah, yeah, exactly. Sometimes there are duplicates of entries in the database. I'm not sure why.

But there are a lot of duplicates in there. In the CMDB itself. Yeah, exactly.

Because on the engage side, our unique identifier is the FTP or the work order. And I did see that sometimes the CMDB database will have duplicate entries for the same work order. But the data is exactly the same.

So there's logic here. Because the duplicate... Some of the records will be only used the latest one. We then process in batches.

So one thing that I do is... I was thinking... So some of our lambdas and some of the Dockerflow lambdas, they will get data from different systems, right? And then they will do a check for each individual document, if that document exists in Couchbase or not. But in this case, you might have... Like, here, it was only 318. But it would still take quite some time to do that individual check, right? But there are times where sometimes, like, at the beginning of the year, you might have like 10,000 or even 20,000 updated records over a weekend or over a day.

Doing that processing individually per doc, it would take a long time. Yeah, so I do it in batches of 500. Because we only want to check if the ID exists.

If it does, give me the latest version. And then we want to do an update. So we use the allbots template for that.

Just do the batch, do the check, and then it's super fast. So instead of a couple of seconds per each request, it's one second per request. Or batch, or two times.

Yeah. So we check in the batch. Batch completed.

Then it will tell you how many documents are going to be uploaded, how many are new, and how many are updated. And then it's going to do the upload. It's successful or not, how many errors are.

And then the results. There's a lot of people who did it, but I'll clean it up. And storing the latest parameters for the last update as well.

That's essentially it. We also have to check and consider if, you know, while it's talking to Sync8, we have to do the uploads. If it fails, if you have to implement a retry or put those in a DLQR.

If, since it's going to be every day, if it's okay for it to, you know, wait for the next run and go from there. That's also something that, yeah. Correct, yeah, let me put that down.

That's a good point, because I don't actually, it hasn't failed yet. So I don't know if, let's say, 13 documents fail, right? Or 200 succeeded. Currently, we don't really retry those 19.

And I'm not, and I think you might actually update this time. So we probably don't want to update that in that case. Yeah, in case of a DLQR.

Yeah, yeah, I think it does. Retry that at least. It's a good point to think about.

So I'll be pretty, like, quiet on getting all the lines. What was that? Who will get all the lines? All the lines, yeah. So that one, I have it so that it will, it will, it will basically decide to just overwrite.

If a 409 happens. So I do have some retry that became there too. Hey, if it already said revision conflict, just create a new revision and add that on top of the old one, right? Yeah, I'm thinking it's pretty weird about the 409.

It's not always just 409, but it's so random. It's like, yeah, I do not understand why sometimes we're getting the 409 that doesn't make any sense. But it's kind of like, you know, a retry will succeed.

We just need to account for, because we know what's going to happen. But we know what's going to happen. Not, not a lot, but it will.

Exactly. All right, I'll take that into account. All right, any more questions on it? So we'll be at, I'll do some logging up here because there's a lot of duplicates.

To me, it actually looks pretty clean. But yeah, once you feel like you've got a little bit of cleaning up, just let us know. It's not going to prod next month, I believe, like mid-December.

So I'm hoping to push to prod, but it's already up in QA, so. Okay, all right. And then patrol is supposed to be going live by end of the year, right? Yes, engage first and then inspect.

I think inspect is supposed to freeze like in the next couple of weeks, and then engage is supposed to freeze in mid-December. We're going to start training, I believe, mid-December or early January. And then the first inspections are going to go, I think, at the third week of January.

This looks like a relatively small piece of the whole thing. How are they doing the inspection? What was that? Of the whole patrol project, right? This looks like a relatively small piece of the whole picture. Yeah, this is pretty long.

That's like probably the simplest piece of the whole thing. It was the simplest piece, but it was the one that took the longest because of the whole power management to the MSC. Yeah.

So how did you end up solving the problem? So it just magically worked. So I'm actually able to send the... It's similar to how we do the EBIR one. However, we are using a service account here.

So that was the main requirement from Cyber, that any integration to any sort of system needs to use a MEA compliant process, as they call it, which means there needs to be an MEA-created service account. But I was worried that because it wasn't just a username and password in the MSSQL database, that it wouldn't work. But they have a simple API that is provided by the MSSQL database to validate a Windows user.

So I send them the credentials, and then they take those credentials and they validate those, I guess, the entry ID locally in the server. Got it. So it's not basic auth, but from our perspective, it's kind of like basic auth.

We don't send them the password in basic auth format. But similar, yeah. It's almost like we're sending... Because when we enter the credentials in the Lambda, the Lambda will almost make a little token of it.

But it's not basic auth credentials. So Lambda is retrieving the parameters, the secrets from the parameters store, and then generating a token from those secrets, and then passing the token to the API for the authentication. I guess we can, Anusha, can you create a story on this? We will also want to make sure to create the documentation for this service.

We need a wiki for this thing. Um, Jonathan, do you know on the patrol side if anyone is thinking of some kind of end-to-end process documentation? This is like one small piece of it. But we need some kind of a view on the entire puzzle.

Let's see. There is a patrol's wiki here. We have some architectural decisions.

So we have the application level architecture when it comes to documents and stuff. We have high-level architecture diagrams for the integration. I'm going to update this one because I have a new one coming up.

But it shows the sort of CFDB Lambda or SYNC service. That's what I call it in this case. We're engaged.

And then there's one for inspect. There's a bunch here. I'll probably check this one.

Not one specifically to the SYNC Lambda. So we'll create it under the address space. Yeah.

But you can see a bunch of stuff there. Okay. Thank you so much.

Yeah, we'll take this up. Nisha, I just want you to create the story. What I would do is I would be working on this.

And I kind of have a shippy shadow me when I work on this. This one looks like a fairly simple one to kind of introduce the concept. What do we do with this kind of stuff? Sure.

Thank you so much. Thanks, Jonathan. Thank you.

Bye-bye. Bye-bye.

(Transcribed by TurboScribe.ai. Go Unlimited to remove this message.)