# Log Forensics Prompt: Targeted Zero-Loss Traceability

Analyze logs for **perfect request tracing** using targeted extraction. Never filterâ€”use `nodrop` always.

## Analysis Method: Chunked Conceptual Understanding

Apply Socratic reasoning to each discovery:
- **Why does this pattern exist?** (reveals logging intent vs reality)
- **How does this relate to cardinality/dimensionality?** (debugging power assessment)
- **What assumptions break?** (traceability gaps)

## Targeted Analysis Framework

### 1. STRUCTURE RECONNAISSANCE (30 seconds)
Scan 3-5 sample events, extract skeleton:

```
STRUCTURE PATTERN:
â”œâ”€ Wrapper: [event/message/record]
â”œâ”€ Service metadata: [name, host, pid, level]
â”œâ”€ Business context: [event type, state, outcome]
â”œâ”€ High-cardinality keys: [list all ID-like fields]
â”œâ”€ Timestamps: [field names + formats]
â””â”€ Nested depth: [count levels]

CARDINALITY QUICK-SCAN:
High (>100 unique): [fields] â†’ correlation candidates
Low (<10 unique): [fields] â†’ pattern grouping
Single-value: [fields] â†’ constants or config
```

**Socratic check:** Why these specific high-cardinality fields? What business entities do they represent?

### 2. CORRELATION POWER TEST
For each ID candidate, assess traceability:

```
IDENTIFIER: flightDetailId
â”œâ”€ Coverage: appears in [X]% of events
â”œâ”€ Uniqueness: [N] distinct values
â”œâ”€ Persistence: appears across [event types]
â”œâ”€ Format: [pattern - UUID/int/composite]
â””â”€ **Power Score:** Can track [N] unique requests across [M] event types

IDENTIFIER: pid  
â”œâ”€ Coverage: [X]%
â”œâ”€ Uniqueness: [N] values (LOW - process ID)
â”œâ”€ **Power Score:** Groups by service instance, not request
â””â”€ Use case: Isolate per-process issues

ORPHAN RISK:
Events without correlation keys: [estimate %]
â”œâ”€ Types affected: [list]
â””â”€ Hypothesis: [initialization events? different service?]
```

**Socratic check:** If `flightDetailId` missing, what composite key works? (user_id + timestamp?)

### 3. FLOW SIGNATURE EXTRACTION
Map actual execution journey from event types:

```
FLOW DISCOVERED: flightDetailId=17364

Step 1: event="Ingest Structures" @ T+0
â”œâ”€ State: success
â”œâ”€ Context: expectedImageCount=18
â”œâ”€ Dimensionality: [N] fields
â””â”€ **Meaning:** Request initiated, expecting 18 images

Step 2: event=[next_type] @ T+[delta]ms
â”œâ”€ New fields that appeared: [list]
â”œâ”€ Changed values: expectedImageCount â†’ actualImageCount
â””â”€ **Meaning:** [what this step does]

Step N: event=[type] @ T+[total]ms
â””â”€ **Outcome:** state=[success/failed]

MISSING STEPS INFERENCE:
â”œâ”€ Gap between step Xâ†’Y: [duration]ms (expected <100ms)
â”œâ”€ Field `actualImageCount` appears in step N but not set in step 1
â””â”€ **Hypothesis:** Image processing step exists but not logging events
```

**Socratic check:** Why does `expectedImageCount` exist if not validated against actual? Hints at missing comparison step.

### 4. FAILURE FINGERPRINT (when present)
Extract maximum context at break point:

```
FAILURE: flightDetailId=17364

Last Success: event="Ingest Structures" @ T+0
â”œâ”€ All context: {name, hostname, pid, level, event, state, flightDetailId, expectedImageCount, stream}

Failure Event: event="Process Images" @ T+2500ms
â”œâ”€ Error dimensions added: [error_code, error_message, stack_trace]
â”œâ”€ **Context diff:**
â”‚   â”œâ”€ expectedImageCount: 18 (unchanged)
â”‚   â”œâ”€ NEW: actualImageCount: 12
â”‚   â””â”€ NEW: error: "Missing 6 images from S3"

ROOT CAUSE SIGNALS (ranked):
1. actualImageCount (12) < expectedImageCount (18)
   â””â”€ **Investigation:** Check S3 upload completeness
2. error message mentions "S3"
   â””â”€ **Investigation:** Check S3 dependency health at T+2500ms
3. 6 missing = 33% failure rate
   â””â”€ **Pattern check:** Does this ratio appear in other failures?
```

**Socratic check:** Why log `expectedImageCount` if not comparing? Suggests defensive logging for debugging.

### 5. DIMENSIONALITY ASSESSMENT
Measure context richness:

```
SUCCESS EVENTS:
â”œâ”€ Avg fields: [N]
â”œâ”€ Always present: [name, hostname, pid, level, event, state, flightDetailId, timestamp]
â””â”€ Sometimes present: [expectedImageCount, imageUrls, processingTime]

FAILURE EVENTS:  
â”œâ”€ Avg fields: [M] (M > N due to error context)
â”œâ”€ Additional dimensions: [error, error_code, error_message, failed_step]
â””â”€ **Gap:** Missing [field] that would show [critical context]

HIGH-VALUE DIMENSIONS:
â”œâ”€ flightDetailId: Isolates specific request
â”œâ”€ event: Shows which step
â”œâ”€ state: Shows outcome
â”œâ”€ expectedImageCount vs actualImageCount: Shows data completeness
â””â”€ **Missing:** Response time, retry count, upstream service ID
```

**Socratic check:** Why is `level: 30` (INFO) static? Failures should elevate to ERROR. Anti-pattern detected.

### 6. WIDE EVENT OPPORTUNITY
Show consolidation target:

```
CURRENT STATE: flightDetailId=17364 generates [N] events

Event 1: "Ingest Structures" (8 fields)
Event 2: "Validate Images" (10 fields)  
Event 3: "Process Images" (12 fields)
Event 4: "Store Results" (9 fields)

WIDE EVENT DESIGN:
{
  // Identity (from all events)
  "flightDetailId": 17364,
  "service": "IngestionService",
  "hostname": "ip-10-90-173-151.us-west-2.compute.internal",
  
  // Flow timeline (from events 1-4)
  "ingest_started_at": "2025-12-18T23:42:51.447",
  "validate_duration_ms": 150,
  "process_duration_ms": 2500,
  "store_duration_ms": 80,
  "total_duration_ms": 2730,
  
  // Business logic (accumulated)
  "expectedImageCount": 18,
  "actualImageCount": 12,
  "processed_images": 12,
  "stored_images": 12,
  
  // Outcome (from final event)
  "state": "partial_success",
  "warnings": ["Missing 6 images"],
  
  // Error context (if failed)
  "error": null
}

BENEFITS:
â”œâ”€ Query complexity: 4 joins â†’ 0 joins
â”œâ”€ Correlation cost: O(4) â†’ O(1)
â””â”€ Context completeness: Immediate visibility into partial failure
```

**Socratic check:** Why 4 events instead of 1? Each step logs independentlyâ€”lacks request-scoped accumulator pattern.

### 7. PATTERN INTELLIGENCE (across all events)
Cross-request insights:

```
SAMPLE SIZE: [N] events, [M] unique flightDetailIds

SUCCESS PATTERN:
â”œâ”€ Avg events per request: [X]
â”œâ”€ Avg duration: [Y]ms (P95: [Z]ms)
â”œâ”€ Common sequence: [event1] â†’ [event2] â†’ [event3]

FAILURE PATTERN:
â”œâ”€ Failure rate: [X]% of requests
â”œâ”€ Common failure point: event="Process Images" ([Y]% of failures)
â”œâ”€ Error distribution:
â”‚   â”œâ”€ "Missing images from S3": [N] occurrences
â”‚   â””â”€ "Timeout": [M] occurrences

CO-OCCURRENCE MATRIX:
When expectedImageCount > 15:
â”œâ”€ Failure rate increases to [X]% (vs [Y]% baseline)
â””â”€ **Hypothesis:** Large batch processing more fragile

When hostname="[specific_host]":
â”œâ”€ [X]% slower than other hosts
â””â”€ **Hypothesis:** Resource contention on that instance
```

**Socratic check:** Why does batch size correlate with failure? Memory pressure? S3 rate limits? Network timeouts?

### 8. ANTI-PATTERN DETECTION
Query complexity = log debt measure:

```
ANTI-PATTERNS FOUND:

1. Static Log Level
   â”œâ”€ Evidence: level=30 for both success and failure
   â”œâ”€ Cost: Can't filter to errors with `level` field
   â””â”€ **Fix:** Set level=ERROR when state="failed"

2. Missing Response Time
   â”œâ”€ Evidence: No duration field in any event
   â”œâ”€ Cost: Can't identify slow requests without manual timestamp diff
   â””â”€ **Fix:** Add processing_duration_ms to each event

3. Multi-Event Request
   â”œâ”€ Evidence: Avg [X] events per flightDetailId
   â”œâ”€ Cost: [X] joins to reconstruct full request
   â””â”€ **Fix:** Emit 1 wide event per request

4. Inconsistent Field Naming
   â”œâ”€ Evidence: `flightDetailId` vs `flight_detail_id` (check full logs)
   â””â”€ **Fix:** Standardize on snake_case or camelCase

QUERY COMPLEXITY SCORE:
â”œâ”€ Correlation: [N] field extractions needed
â”œâ”€ Flow reconstruction: [M] timestamp normalizations
â”œâ”€ Context assembly: [X] joins per request
â””â”€ **Grade: [A-F]** (A = single query, F = >5 operations)
```

**Socratic check:** Why does complexity exist? Reveals incremental logging additions without architectural review.

## Output Format

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FORENSIC ANALYSIS: [Service Name]
Events: [N] | Requests: [M] | Timespan: [duration]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š STRUCTURE
â””â”€ [skeleton with cardinality tags]

ğŸ”— CORRELATION  
â”œâ”€ Primary: flightDetailId ([N] requests, [X]% coverage)
â””â”€ Orphans: [Y]% untraceable

âš¡ FLOW SIGNATURE
â””â”€ [typical sequence with timings]

âŒ FAILURE PATTERN (if present)
â”œâ”€ flightDetailId=[X] failed at [step]
â”œâ”€ Root cause: [field changes]
â””â”€ Investigation: [specific path]

ğŸ“ˆ PATTERN INSIGHTS
â””â”€ [cross-request discoveries]

ğŸ“¦ WIDE EVENT TARGET
â””â”€ [N] events â†’ 1 event with [M] fields

âš ï¸ ANTI-PATTERNS
â””â”€ [ranked by impact on traceability]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ ACTIONABLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DEBUG [failure]: correlation_id=[X] â†’ investigate [Y]
PREVENT: [pattern] â†’ fix [Z]
IMPROVE: Add [field] â†’ enable [capability]
```

## Critical Rules

1. **NEVER filter** - use `nodrop`, analyze all events
2. **Concrete values** - actual flightDetailId=17364, not "[ID]"
3. **Quantify impact** - "[X]% faster" not "better"
4. **Socratic reasoning** - explain WHY patterns matter
5. **Cardinality awareness** - high=isolation, low=grouping
6. **Dimensionality maximization** - more fields = more debugging angles
7. **Wide event thinking** - always show consolidation opportunity

Now analyze the provided logs.
